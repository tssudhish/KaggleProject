{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_top\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
    "</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><h1>Pre-trained-Models with PyTorch </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n",
    "<ul>\n",
    "<li>change the output layer</li>\n",
    "<li> train the model</li> \n",
    "<li>  identify  several  misclassified samples</li> \n",
    " </ul>\n",
    "You will take several screenshots of your work and share your notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#download_data\"> Download Data</a></li>\n",
    "    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
    "    <li><a href=\"#data_class\"> Dataset Class</a></li>\n",
    "    <li><a href=\"#Question_1\">Question 1</a></li>\n",
    "    <li><a href=\"#Question_2\">Question 2</a></li>\n",
    "    <li><a href=\"#Question_3\">Question 3</a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
    " </div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"download_data\">Download Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-14 20:46:11--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2598656062 (2.4G) [application/zip]\n",
      "Saving to: ‘Positive_tensors.zip’\n",
      "\n",
      "100%[====================================>] 2,598,656,062 46.4MB/s   in 59s    \n",
      "\n",
      "2020-08-14 20:47:10 (42.0 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q Positive_tensors.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-14 20:49:25--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2111408108 (2.0G) [application/zip]\n",
      "Saving to: ‘Negative_tensors.zip’\n",
      "\n",
      "100%[====================================>] 2,111,408,108 44.5MB/s   in 48s    \n",
      "\n",
      "2020-08-14 20:50:14 (41.7 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
    "!unzip -q Negative_tensors.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will install torchvision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/dc/4a939cfbd38398f4765f712576df21425241020bfccc200af76d19088533/torchvision-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (5.9MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9MB 7.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (1.15.4)\n",
      "Collecting torch==1.6.0 (from torchvision)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/53/914885a93a44b96c0dd1c36f36ff10afe341f091230aad68f7228d61db1e/torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
      "\u001b[K     |████████████████████████████████| 748.8MB 22kB/s s eta 0:00:010��████▏                       | 190.4MB 12.8MB/s eta 0:00:44     |██████████▎                     | 239.4MB 16.5MB/s eta 0:00:31███████████████▉          | 509.7MB 12.9MB/s eta 0:00:19     |█████████████████████████████▊  | 696.5MB 29.8MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (5.4.1)\n",
      "Requirement already satisfied: future in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torch==1.6.0->torchvision) (0.17.1)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-1.6.0 torchvision-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f59407562d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the libraries will be used for this lab.\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import pandas\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch \n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import os\n",
    "import glob\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pylab as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"data_class\">Dataset Class</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Create your own dataset object\n",
    "\n",
    "class Dataset(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self,transform=None,train=True):\n",
    "        directory=\"/home/dsxuser/work\"\n",
    "        positive=\"Positive_tensors\"\n",
    "        negative='Negative_tensors'\n",
    "\n",
    "        positive_file_path=os.path.join(directory,positive)\n",
    "        negative_file_path=os.path.join(directory,negative)\n",
    "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
    "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
    "        number_of_samples=len(positive_files)+len(negative_files)\n",
    "        self.all_files=[None]*number_of_samples\n",
    "        self.all_files[::2]=positive_files\n",
    "        self.all_files[1::2]=negative_files \n",
    "        # The transform is goint to be used on image\n",
    "        self.transform = transform\n",
    "        #torch.LongTensor\n",
    "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
    "        self.Y[::2]=1\n",
    "        self.Y[1::2]=0\n",
    "        \n",
    "        if train:\n",
    "            self.all_files=self.all_files[0:30000]\n",
    "            self.Y=self.Y[0:30000]\n",
    "            self.len=len(self.all_files)\n",
    "        else:\n",
    "            self.all_files=self.all_files[30000:]\n",
    "            self.Y=self.Y[30000:]\n",
    "            self.len=len(self.all_files)     \n",
    "       \n",
    "    # Get the length\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    # Getter\n",
    "    def __getitem__(self, idx):\n",
    "               \n",
    "        image=torch.load(self.all_files[idx])\n",
    "        y=self.Y[idx]\n",
    "                  \n",
    "        # If there is any transform method, apply it onto the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, y\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two dataset objects, one for the training data and one for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(train=True)\n",
    "validation_dataset = Dataset(train=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Question_1\">Question 1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Prepare a pre-trained resnet18 model :</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/dsxuser/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c22594350cd40978304057debb8521e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=46827520), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the pre-trained model resnet18\n",
    "\n",
    "model=models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
    "\n",
    "\n",
    "model.requires_grad_=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_input_features=model.fc.in_features\n",
    "model.fc=nn.Linear(original_input_features,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Question_2\">Question 2: Train the Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question you will train your, model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1</b>: Create a cross entropy criterion function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the loss function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=100)\n",
    "validation_loader=DataLoader(validation_dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3</b>: Use the following optimizer to minimize the loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_test = 10000\n",
      "N_train = 30000\n",
      "Batch Number 0 of training\n",
      "Batch Number 1 of training\n",
      "Batch Number 2 of training\n",
      "Batch Number 3 of training\n",
      "Batch Number 4 of training\n",
      "Batch Number 5 of training\n",
      "Batch Number 6 of training\n",
      "Batch Number 7 of training\n",
      "Batch Number 8 of training\n",
      "Batch Number 9 of training\n",
      "Batch Number 10 of training\n",
      "Batch Number 11 of training\n",
      "Batch Number 12 of training\n",
      "Batch Number 13 of training\n",
      "Batch Number 14 of training\n",
      "Batch Number 15 of training\n",
      "Batch Number 16 of training\n",
      "Batch Number 17 of training\n",
      "Batch Number 18 of training\n",
      "Batch Number 19 of training\n",
      "Batch Number 20 of training\n",
      "Batch Number 21 of training\n",
      "Batch Number 22 of training\n",
      "Batch Number 23 of training\n",
      "Batch Number 24 of training\n",
      "Batch Number 25 of training\n",
      "Batch Number 26 of training\n",
      "Batch Number 27 of training\n",
      "Batch Number 28 of training\n",
      "Batch Number 29 of training\n",
      "Batch Number 30 of training\n",
      "Batch Number 31 of training\n",
      "Batch Number 32 of training\n",
      "Batch Number 33 of training\n",
      "Batch Number 34 of training\n",
      "Batch Number 35 of training\n",
      "Batch Number 36 of training\n",
      "Batch Number 37 of training\n",
      "Batch Number 38 of training\n",
      "Batch Number 39 of training\n",
      "Batch Number 40 of training\n",
      "Batch Number 41 of training\n",
      "Batch Number 42 of training\n",
      "Batch Number 43 of training\n",
      "Batch Number 44 of training\n",
      "Batch Number 45 of training\n",
      "Batch Number 46 of training\n",
      "Batch Number 47 of training\n",
      "Batch Number 48 of training\n",
      "Batch Number 49 of training\n",
      "Batch Number 50 of training\n",
      "Batch Number 51 of training\n",
      "Batch Number 52 of training\n",
      "Batch Number 53 of training\n",
      "Batch Number 54 of training\n",
      "Batch Number 55 of training\n",
      "Batch Number 56 of training\n",
      "Batch Number 57 of training\n",
      "Batch Number 58 of training\n",
      "Batch Number 59 of training\n",
      "Batch Number 60 of training\n",
      "Batch Number 61 of training\n",
      "Batch Number 62 of training\n",
      "Batch Number 63 of training\n",
      "Batch Number 64 of training\n",
      "Batch Number 65 of training\n",
      "Batch Number 66 of training\n",
      "Batch Number 67 of training\n",
      "Batch Number 68 of training\n",
      "Batch Number 69 of training\n",
      "Batch Number 70 of training\n",
      "Batch Number 71 of training\n",
      "Batch Number 72 of training\n",
      "Batch Number 73 of training\n",
      "Batch Number 74 of training\n",
      "Batch Number 75 of training\n",
      "Batch Number 76 of training\n",
      "Batch Number 77 of training\n",
      "Batch Number 78 of training\n",
      "Batch Number 79 of training\n",
      "Batch Number 80 of training\n",
      "Batch Number 81 of training\n",
      "Batch Number 82 of training\n",
      "Batch Number 83 of training\n",
      "Batch Number 84 of training\n",
      "Batch Number 85 of training\n",
      "Batch Number 86 of training\n",
      "Batch Number 87 of training\n",
      "Batch Number 88 of training\n",
      "Batch Number 89 of training\n",
      "Batch Number 90 of training\n",
      "Batch Number 91 of training\n",
      "Batch Number 92 of training\n",
      "Batch Number 93 of training\n",
      "Batch Number 94 of training\n",
      "Batch Number 95 of training\n",
      "Batch Number 96 of training\n",
      "Batch Number 97 of training\n",
      "Batch Number 98 of training\n",
      "Batch Number 99 of training\n",
      "Batch Number 100 of training\n",
      "Batch Number 101 of training\n",
      "Batch Number 102 of training\n",
      "Batch Number 103 of training\n",
      "Batch Number 104 of training\n",
      "Batch Number 105 of training\n",
      "Batch Number 106 of training\n",
      "Batch Number 107 of training\n",
      "Batch Number 108 of training\n",
      "Batch Number 109 of training\n",
      "Batch Number 110 of training\n",
      "Batch Number 111 of training\n",
      "Batch Number 112 of training\n",
      "Batch Number 113 of training\n",
      "Batch Number 114 of training\n",
      "Batch Number 115 of training\n",
      "Batch Number 116 of training\n",
      "Batch Number 117 of training\n",
      "Batch Number 118 of training\n",
      "Batch Number 119 of training\n",
      "Batch Number 120 of training\n",
      "Batch Number 121 of training\n",
      "Batch Number 122 of training\n",
      "Batch Number 123 of training\n",
      "Batch Number 124 of training\n",
      "Batch Number 125 of training\n",
      "Batch Number 126 of training\n",
      "Batch Number 127 of training\n",
      "Batch Number 128 of training\n",
      "Batch Number 129 of training\n",
      "Batch Number 130 of training\n",
      "Batch Number 131 of training\n",
      "Batch Number 132 of training\n",
      "Batch Number 133 of training\n",
      "Batch Number 134 of training\n",
      "Batch Number 135 of training\n",
      "Batch Number 136 of training\n",
      "Batch Number 137 of training\n",
      "Batch Number 138 of training\n",
      "Batch Number 139 of training\n",
      "Batch Number 140 of training\n",
      "Batch Number 141 of training\n",
      "Batch Number 142 of training\n",
      "Batch Number 143 of training\n",
      "Batch Number 144 of training\n",
      "Batch Number 145 of training\n",
      "Batch Number 146 of training\n",
      "Batch Number 147 of training\n",
      "Batch Number 148 of training\n",
      "Batch Number 149 of training\n",
      "Batch Number 150 of training\n",
      "Batch Number 151 of training\n",
      "Batch Number 152 of training\n",
      "Batch Number 153 of training\n",
      "Batch Number 154 of training\n",
      "Batch Number 155 of training\n",
      "Batch Number 156 of training\n",
      "Batch Number 157 of training\n",
      "Batch Number 158 of training\n",
      "Batch Number 159 of training\n",
      "Batch Number 160 of training\n",
      "Batch Number 161 of training\n",
      "Batch Number 162 of training\n",
      "Batch Number 163 of training\n",
      "Batch Number 164 of training\n",
      "Batch Number 165 of training\n",
      "Batch Number 166 of training\n",
      "Batch Number 167 of training\n",
      "Batch Number 168 of training\n",
      "Batch Number 169 of training\n",
      "Batch Number 170 of training\n",
      "Batch Number 171 of training\n",
      "Batch Number 172 of training\n",
      "Batch Number 173 of training\n",
      "Batch Number 174 of training\n",
      "Batch Number 175 of training\n",
      "Batch Number 176 of training\n",
      "Batch Number 177 of training\n",
      "Batch Number 178 of training\n",
      "Batch Number 179 of training\n",
      "Batch Number 180 of training\n",
      "Batch Number 181 of training\n",
      "Batch Number 182 of training\n",
      "Batch Number 183 of training\n",
      "Batch Number 184 of training\n",
      "Batch Number 185 of training\n",
      "Batch Number 186 of training\n",
      "Batch Number 187 of training\n",
      "Batch Number 188 of training\n",
      "Batch Number 189 of training\n",
      "Batch Number 190 of training\n",
      "Batch Number 191 of training\n",
      "Batch Number 192 of training\n",
      "Batch Number 193 of training\n",
      "Batch Number 194 of training\n",
      "Batch Number 195 of training\n",
      "Batch Number 196 of training\n",
      "Batch Number 197 of training\n",
      "Batch Number 198 of training\n",
      "Batch Number 199 of training\n",
      "Batch Number 200 of training\n",
      "Batch Number 201 of training\n",
      "Batch Number 202 of training\n",
      "Batch Number 203 of training\n",
      "Batch Number 204 of training\n",
      "Batch Number 205 of training\n",
      "Batch Number 206 of training\n",
      "Batch Number 207 of training\n",
      "Batch Number 208 of training\n",
      "Batch Number 209 of training\n",
      "Batch Number 210 of training\n",
      "Batch Number 211 of training\n",
      "Batch Number 212 of training\n",
      "Batch Number 213 of training\n",
      "Batch Number 214 of training\n",
      "Batch Number 215 of training\n",
      "Batch Number 216 of training\n",
      "Batch Number 217 of training\n",
      "Batch Number 218 of training\n",
      "Batch Number 219 of training\n",
      "Batch Number 220 of training\n",
      "Batch Number 221 of training\n",
      "Batch Number 222 of training\n",
      "Batch Number 223 of training\n",
      "Batch Number 224 of training\n",
      "Batch Number 225 of training\n",
      "Batch Number 226 of training\n",
      "Batch Number 227 of training\n",
      "Batch Number 228 of training\n",
      "Batch Number 229 of training\n",
      "Batch Number 230 of training\n",
      "Batch Number 231 of training\n",
      "Batch Number 232 of training\n",
      "Batch Number 233 of training\n",
      "Batch Number 234 of training\n",
      "Batch Number 235 of training\n",
      "Batch Number 236 of training\n",
      "Batch Number 237 of training\n",
      "Batch Number 238 of training\n",
      "Batch Number 239 of training\n",
      "Batch Number 240 of training\n",
      "Batch Number 241 of training\n",
      "Batch Number 242 of training\n",
      "Batch Number 243 of training\n",
      "Batch Number 244 of training\n",
      "Batch Number 245 of training\n",
      "Batch Number 246 of training\n",
      "Batch Number 247 of training\n",
      "Batch Number 248 of training\n",
      "Batch Number 249 of training\n",
      "Batch Number 250 of training\n",
      "Batch Number 251 of training\n",
      "Batch Number 252 of training\n",
      "Batch Number 253 of training\n",
      "Batch Number 254 of training\n",
      "Batch Number 255 of training\n",
      "Batch Number 256 of training\n",
      "Batch Number 257 of training\n",
      "Batch Number 258 of training\n",
      "Batch Number 259 of training\n",
      "Batch Number 260 of training\n",
      "Batch Number 261 of training\n",
      "Batch Number 262 of training\n",
      "Batch Number 263 of training\n",
      "Batch Number 264 of training\n",
      "Batch Number 265 of training\n",
      "Batch Number 266 of training\n",
      "Batch Number 267 of training\n",
      "Batch Number 268 of training\n",
      "Batch Number 269 of training\n",
      "Batch Number 270 of training\n",
      "Batch Number 271 of training\n",
      "Batch Number 272 of training\n",
      "Batch Number 273 of training\n",
      "Batch Number 274 of training\n",
      "Batch Number 275 of training\n",
      "Batch Number 276 of training\n",
      "Batch Number 277 of training\n",
      "Batch Number 278 of training\n",
      "Batch Number 279 of training\n",
      "Batch Number 280 of training\n",
      "Batch Number 281 of training\n",
      "Batch Number 282 of training\n",
      "Batch Number 283 of training\n",
      "Batch Number 284 of training\n",
      "Batch Number 285 of training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Number 286 of training\n",
      "Batch Number 287 of training\n",
      "Batch Number 288 of training\n",
      "Batch Number 289 of training\n",
      "Batch Number 290 of training\n",
      "Batch Number 291 of training\n",
      "Batch Number 292 of training\n",
      "Batch Number 293 of training\n",
      "Batch Number 294 of training\n",
      "Batch Number 295 of training\n",
      "Batch Number 296 of training\n",
      "Batch Number 297 of training\n",
      "Batch Number 298 of training\n",
      "Batch Number 299 of training\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "n_epochs=1\n",
    "loss_list=[]\n",
    "accuracy_list=[]\n",
    "correct=0\n",
    "N_test=len(validation_dataset)\n",
    "print(\"N_test = {}\".format(N_test))\n",
    "\n",
    "N_train=len(train_dataset)\n",
    "print(\"N_train = {}\".format(N_train))\n",
    "start_time = time.time()\n",
    "#n_epochs\n",
    "\n",
    "Loss=0\n",
    "start_time = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    counter=0\n",
    "    for x, y in train_loader:\n",
    "        print(\"Batch Number {} of training\".format(counter))\n",
    "        # model train\n",
    "        model.train() \n",
    "        # clear gradient\n",
    "        optimizer.zero_grad()\n",
    "        # make a prediction\n",
    "        yhat=model(x)\n",
    "\n",
    "        # calculate loss \n",
    "        loss=criterion(yhat, y)\n",
    "        # calculate gradients of parameters\n",
    "        loss.backward()\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.data)\n",
    "        counter+=1\n",
    "\n",
    "    correct=0\n",
    "    for x_test, y_test in validation_loader:\n",
    "        # set model to eval\n",
    "        model.eval()\n",
    "        #make a prediction\n",
    "        prediction=model(x_test)\n",
    "        # find max \n",
    "        _,yhat=torch.max(prediction,1)\n",
    "        #print(type(yhat))\n",
    "        # find max \n",
    "        #Calculate misclassified  samples in mini-batch \n",
    "        accuracy_list+=list(map(int,[y1==y2 for y1,y2 in zip(yhat,y_test)]))\n",
    "        \n",
    "    correct=sum(accuracy_list)\n",
    "    accuracy=correct/N_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9967"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Identify the first four misclassified samples using the validation data:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all misclassified images will have accuracy value of 0 in the accuracy_list\n",
    "misclassified_image_index=[idx for idx,val in enumerate(accuracy_list) if val == 0]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 367 predicted value: tensor([1]) actual value:tensor([0])\n",
      "sample 403 predicted value: tensor([1]) actual value:tensor([0])\n",
      "sample 444 predicted value: tensor([0]) actual value:tensor([1])\n",
      "sample 879 predicted value: tensor([1]) actual value:tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# first four misclassified images \n",
    "for idx in range(4):\n",
    "    sample_idx=misclassified_image_index[idx]\n",
    "    image,y = validation_dataset[sample_idx]\n",
    "    x=image.unsqueeze(0)\n",
    "    y=y.unsqueeze(0)\n",
    "    prediction=model(x)\n",
    "    _,yhat=torch.max(prediction,1)\n",
    "    print(\"sample {} predicted value: {} actual value:{}\".format(sample_idx,yhat,y))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--- --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html\"> CLICK HERE </a> Click here to see how to share your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
